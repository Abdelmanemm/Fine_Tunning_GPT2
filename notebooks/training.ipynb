{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Essential Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Version: 12.1\n",
      "Is CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA Version:\", torch.version.cuda)\n",
    "print(\"Is CUDA available:\", torch.cuda.is_available())\n",
    "from transformers import GPT2Tokenizer , GPT2LMHeadModel , TrainingArguments , Trainer , DataCollatorForLanguageModeling\n",
    "from torch.utils.data import Dataset , DataLoader\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PreProcess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/Workspace/Prodigy_InfoTech_Internship/task1-Text_Genration_with_gpt2/src/task1/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "class JokesDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, num_samples):\n",
    "        # read data file\n",
    "        self.data = pd.read_csv(file_path)\n",
    "        # get random samples from data file that is == to number of samples\n",
    "        # we want to use to train model\n",
    "        if num_samples and num_samples < len(self.data):\n",
    "            self.data = self.data.sample(n=num_samples, random_state=42).reset_index(drop=True)\n",
    "        # save the jokes in a list\n",
    "        self.jokes = self.data[\"joke\"].tolist()\n",
    "        # initialize tokenizer\n",
    "        self.tokenizer = tokenizer\n",
    "        # tokenize jokes and save output to be input to the model in training phase\n",
    "        self.inputs = self.tokenizer(self.jokes, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    \n",
    "    def __len__(self):\n",
    "        # return no. of  jokes in the dataset\n",
    "        return len(self.jokes)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # tokenized text\n",
    "        input_ids = self.inputs['input_ids'][idx]\n",
    "        # indicate which token is an actual token and which is a padded token\n",
    "        attention_mask = self.inputs['attention_mask'][idx]\n",
    "        return {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "\n",
    "# Load Tokenizer and set a pad token\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Create a dataset instance \n",
    "dataset = JokesDataset(file_path=\"/mnt/d/Workspace/Prodigy_InfoTech_Internship/task1-Text_Genration_with_gpt2/data/Jokes.csv\", tokenizer=tokenizer, num_samples=20000)\n",
    "\n",
    "# Create a data loader\n",
    "data_loader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512]) torch.Size([2, 512])\n"
     ]
    }
   ],
   "source": [
    "# test that data loader has created correct shapes\n",
    "for batch in data_loader:\n",
    "    input_ids = batch['input_ids']\n",
    "    attention_mask = batch['attention_mask']\n",
    "    print(input_ids.shape,attention_mask.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/Workspace/Prodigy_InfoTech_Internship/task1-Text_Genration_with_gpt2/src/task1/lib/python3.12/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Import prtrained gpt2-medium (124M parameters)\n",
    "gpt2_medium = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n",
    "\n",
    "# Initialize training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=200,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "# Create data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer,mlm=False)\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=gpt2_medium,\n",
    "    args=training_args,\n",
    "    data_collator= data_collator,\n",
    "    train_dataset=dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30000' max='30000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30000/30000 2:33:21, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.420900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.482500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.399400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.437100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>3.334700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>3.413100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>3.410200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>3.350200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.443400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>3.413200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>3.408700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>3.412300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>3.320800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.265700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>3.377000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>3.216600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>3.433600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>3.338300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>3.270200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>3.247100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>3.241800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>3.219800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>3.268300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>3.326000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>3.265700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>3.257900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>3.292500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>3.251300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>3.260200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>3.243900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>3.228500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>3.186000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>3.254800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>3.272300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>3.256500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>3.203100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>3.276900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>3.226900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>3.146300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>3.124800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>3.209500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>3.169300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>3.158500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>3.153700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>3.281300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>3.196200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>3.187800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>3.159200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.192000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>2.498900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>2.544000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>2.567500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>2.527100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.510700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>2.596400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>2.630700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>2.578000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>2.561800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>2.503300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>2.539900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>2.539200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>2.547900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>2.563500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>2.686600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>2.575500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>2.523300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>2.565500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>2.527200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>2.534000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14200</td>\n",
       "      <td>2.494400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>2.497700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14600</td>\n",
       "      <td>2.568300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14800</td>\n",
       "      <td>2.541800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>2.525600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15200</td>\n",
       "      <td>2.572200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15400</td>\n",
       "      <td>2.605000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15600</td>\n",
       "      <td>2.572900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15800</td>\n",
       "      <td>2.563600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>2.559900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16200</td>\n",
       "      <td>2.558300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16400</td>\n",
       "      <td>2.527700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16600</td>\n",
       "      <td>2.518100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16800</td>\n",
       "      <td>2.510500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>2.521400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17200</td>\n",
       "      <td>2.616500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17400</td>\n",
       "      <td>2.542300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17600</td>\n",
       "      <td>2.573200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17800</td>\n",
       "      <td>2.610800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>2.572400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18200</td>\n",
       "      <td>2.562900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18400</td>\n",
       "      <td>2.552200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18600</td>\n",
       "      <td>2.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18800</td>\n",
       "      <td>2.595200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>2.562000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19200</td>\n",
       "      <td>2.526700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19400</td>\n",
       "      <td>2.519300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>2.542800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19800</td>\n",
       "      <td>2.559800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>2.541500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200</td>\n",
       "      <td>2.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20400</td>\n",
       "      <td>2.056100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20600</td>\n",
       "      <td>2.034700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20800</td>\n",
       "      <td>2.066900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>2.096200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21200</td>\n",
       "      <td>2.088500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21400</td>\n",
       "      <td>2.103600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21600</td>\n",
       "      <td>2.103400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21800</td>\n",
       "      <td>2.139000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>2.089500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22200</td>\n",
       "      <td>2.094200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22400</td>\n",
       "      <td>2.064400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22600</td>\n",
       "      <td>2.072700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22800</td>\n",
       "      <td>2.043500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>2.046200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23200</td>\n",
       "      <td>2.089200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23400</td>\n",
       "      <td>2.071800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23600</td>\n",
       "      <td>2.077500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23800</td>\n",
       "      <td>2.049000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>2.063600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24200</td>\n",
       "      <td>2.076900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24400</td>\n",
       "      <td>2.105100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24600</td>\n",
       "      <td>2.054900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24800</td>\n",
       "      <td>2.114100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>2.087600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25200</td>\n",
       "      <td>2.101900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25400</td>\n",
       "      <td>2.077200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25600</td>\n",
       "      <td>2.060800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25800</td>\n",
       "      <td>2.103700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>2.058000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26200</td>\n",
       "      <td>2.119800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26400</td>\n",
       "      <td>2.090200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26600</td>\n",
       "      <td>2.117400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26800</td>\n",
       "      <td>2.084100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>2.084200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27200</td>\n",
       "      <td>2.059500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27400</td>\n",
       "      <td>2.082500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27600</td>\n",
       "      <td>2.097200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27800</td>\n",
       "      <td>2.075900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>2.070400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28200</td>\n",
       "      <td>2.071800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28400</td>\n",
       "      <td>2.024700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28600</td>\n",
       "      <td>2.119500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28800</td>\n",
       "      <td>2.092000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>2.059400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29200</td>\n",
       "      <td>2.107000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29400</td>\n",
       "      <td>2.060100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29600</td>\n",
       "      <td>2.052300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29800</td>\n",
       "      <td>2.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>2.087500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=30000, training_loss=2.641936144002279, metrics={'train_runtime': 9202.4373, 'train_samples_per_second': 6.52, 'train_steps_per_second': 3.26, 'total_flos': 5.572204167168e+16, 'train_loss': 2.641936144002279, 'epoch': 3.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "task1gpt2",
   "language": "python",
   "name": "task1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
